{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import vgg16_network as net\n",
    "import read_img\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE_BASE = 0.001\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0002\n",
    "MAX_ITER = 40000\n",
    "BATCH_SIZE=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_checkpoint_dir = '/path/to/save_model/Driver_Detection_ver8/'\n",
    "read_checkpoint_dir = '/path/to/save_model/Driver_Detection_ver4/'\n",
    "ckt_name = 'model.ckpt-14010'\n",
    "#trainc0 ver5 'model.ckpt-14010'\n",
    "#trainc1 ver6 'model.ckpt-5826'\n",
    "#trainc2 ver7'model.ckpt-10329'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_val_data(data,pre_test=0.1,random_val=True):\n",
    "    if random_val:\n",
    "        data_c0 = data[np.where(data[:,1]=='c0')[0]]\n",
    "        data_c1 = data[np.where(data[:,1]=='c1')[0]]\n",
    "        data_c2 = data[np.where(data[:,1]=='c2')[0]]\n",
    "        data_c3 = data[np.where(data[:,1]=='c3')[0]]\n",
    "        data_c4 = data[np.where(data[:,1]=='c4')[0]]\n",
    "        data_c5 = data[np.where(data[:,1]=='c5')[0]]\n",
    "        data_c6 = data[np.where(data[:,1]=='c6')[0]]\n",
    "        data_c7 = data[np.where(data[:,1]=='c7')[0]]\n",
    "        data_c8 = data[np.where(data[:,1]=='c8')[0]]\n",
    "        data_c9 = data[np.where(data[:,1]=='c9')[0]]\n",
    "\n",
    "        train_c0,test_c0,__,___ = train_test_split(data_c0,np.zeros(data_c0.shape[0]),test_size =pre_test)\n",
    "        train_c1,test_c1,__,___ = train_test_split(data_c1,np.zeros(data_c1.shape[0]),test_size =pre_test)\n",
    "        train_c2,test_c2,__,___ = train_test_split(data_c2,np.zeros(data_c2.shape[0]),test_size =pre_test)\n",
    "        train_c3,test_c3,__,___ = train_test_split(data_c3,np.zeros(data_c3.shape[0]),test_size =pre_test)\n",
    "        train_c4,test_c4,__,___ = train_test_split(data_c4,np.zeros(data_c4.shape[0]),test_size =pre_test)\n",
    "        train_c5,test_c5,__,___ = train_test_split(data_c5,np.zeros(data_c5.shape[0]),test_size =pre_test)\n",
    "        train_c6,test_c6,__,___ = train_test_split(data_c6,np.zeros(data_c6.shape[0]),test_size =pre_test)\n",
    "        train_c7,test_c7,__,___ = train_test_split(data_c7,np.zeros(data_c7.shape[0]),test_size =pre_test)\n",
    "        train_c8,test_c8,__,___ = train_test_split(data_c8,np.zeros(data_c8.shape[0]),test_size =pre_test)\n",
    "        train_c9,test_c9,__,___ = train_test_split(data_c9,np.zeros(data_c9.shape[0]),test_size =pre_test)\n",
    "        train_data = np.concatenate((train_c0,train_c1,train_c2,train_c3,train_c4,train_c5,train_c6,train_c7,train_c8,train_c9))\n",
    "        test_data = np.concatenate((test_c0,test_c1,test_c2,test_c3,test_c4,test_c5,test_c6,test_c7,test_c8,test_c9))\n",
    "    else:\n",
    "        subject_name = list(set(data[:,0]))\n",
    "        train_c0,test_c0,__,___ = train_test_split(subject_name,np.zeros(len(subject_name)),test_size =0.15)\n",
    "        train_c0 = [ 'p042','p012', 'p072', 'p024','p064','p051', 'p081', 'p022',  'p061', 'p066', 'p002', 'p015', 'p052', 'p014', 'p026', 'p056', 'p049', 'p050', 'p035', 'p075', 'p047', 'p039']\n",
    "        test_c0 = ['p021', 'p041', 'p016', 'p045']\n",
    "        train_data = data[np.where(data[:,0]==train_c0[0])[0]]\n",
    "        for i in range(1,len(train_c0)):\n",
    "            cc = data[np.where(data[:,0]==train_c0[i])[0]]\n",
    "            train_data = np.concatenate((train_data,cc))\n",
    "\n",
    "        test_data = data[np.where(data[:,0]==test_c0[0])[0]]\n",
    "        for i in range(1,len(test_c0)):\n",
    "            cc = data[np.where(data[:,0]==test_c0[i])[0]]\n",
    "            test_data = np.concatenate((test_data,cc))\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(test_data)\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(dataAug=False,checkpoint=False):    \n",
    "    input_data = tf.placeholder(tf.float32,shape=[BATCH_SIZE,None,None,3],name='img_input')\n",
    "    label = tf.placeholder(tf.int32,shape=[None],name='label')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    learning_rate = tf.placeholder(tf.float32,name='learning_rate')\n",
    "    L2_regu_rate = tf.placeholder(tf.float32,name='L2_regu_rate')\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(L2_regu_rate)\n",
    "    if dataAug:\n",
    "        vgg_input = read_img.tfDataAug(input_data,batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        vgg_input = input_data\n",
    "    cls_10 = net.VGG16(vgg_input,keep_prob,regularizer,checkpoint=checkpoint,checkpoint_dir=read_checkpoint_dir,ckt_name=ckt_name)\n",
    "    cls_10_softmax = tf.nn.softmax(cls_10)\n",
    "    cls_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_10,labels=label))\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False,name=\"global_step\")\n",
    "    loss = cls_loss + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    #learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,1000, LEARNING_RATE_DECAY,staircase=True)\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    #train_op = tf.train.MomentumOptimizer(learning_rate,MOMENTUM).minimize(loss,global_step=global_step)\n",
    "    #train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    \n",
    "    tf.summary.scalar('all_loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    merged = tf.summary.merge_all()\n",
    "    #################################\n",
    "    saver = tf.train.Saver()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        summary_writer = tf.summary.FileWriter(r\"/path/to/Driver_Detection\",sess.graph)\n",
    "        tf.global_variables_initializer().run()\n",
    "        if checkpoint:\n",
    "            n_StartCrossVal=0\n",
    "        else:\n",
    "            n_StartCrossVal=5\n",
    "        count =0\n",
    "        start = 0\n",
    "        end = BATCH_SIZE\n",
    "        #driver_imgs_list = pd.read_csv('data/driver_imgs_list.csv')\n",
    "        driver_imgs_list = pd.read_csv('data/Aug_driver_imgs_list.csv')\n",
    "        train_data,test_data = train_val_data(driver_imgs_list.values,pre_test=0.1,random_val=True)\n",
    "        train_data_len = train_data.shape[0]\n",
    "        n_batch = train_data_len//BATCH_SIZE\n",
    "        accury_max = 0\n",
    "        num_acc_noupdate=0\n",
    "        pre_train_num = 0\n",
    "        val_loss = 20\n",
    "        while count<MAX_ITER:\n",
    "            if num_acc_noupdate>10:break\n",
    "            img =[]\n",
    "            img_label = []\n",
    "            for i in range(start,end):\n",
    "                i = i%train_data_len\n",
    "                img_file = os.path.join('data','train', train_data[i,1]) + '\\\\'+ train_data[i,2]\n",
    "                img_tmp = read_img.get_img_sample(img_file,dataAug=dataAug)\n",
    "                label_tmp = [int(train_data[i,1][1])]\n",
    "                img.append(img_tmp)\n",
    "                img_label.append(label_tmp)\n",
    "            img = np.array(img).reshape(BATCH_SIZE,img_tmp.shape[1],img_tmp.shape[2],3)\n",
    "            img_label = np.array(img_label).reshape(BATCH_SIZE)\n",
    "            start = (start+BATCH_SIZE)%train_data_len\n",
    "            end   = (end+BATCH_SIZE)%train_data_len\n",
    "            if(end<start): end = start+BATCH_SIZE\n",
    "            if count<=(n_StartCrossVal)*n_batch:\n",
    "                lr = LEARNING_RATE_BASE\n",
    "                l2_rate= REGULARAZTION_RATE\n",
    "            else:\n",
    "                lr = LEARNING_RATE_BASE*0.1\n",
    "                l2_rate= REGULARAZTION_RATE*0.1\n",
    "            summary,_t_,loss_run,pre_cls,step = sess.run([merged,train_op,loss,cls_10_softmax,global_step],\n",
    "                        feed_dict={input_data:img,label:img_label,keep_prob:0.5, learning_rate:lr,L2_regu_rate:l2_rate})\n",
    "            pre_label = np.argmax(pre_cls,axis=1)\n",
    "            tmp_num = np.where(pre_label==img_label)[0].shape[0]\n",
    "            pre_train_num += tmp_num\n",
    "            #if count%400==0: print('step',step,\"loss\",loss_run)\n",
    "            if (count+1)%n_batch==0:\n",
    "                print(\"train acc is \",pre_train_num/(n_batch*BATCH_SIZE))\n",
    "                pre_train_num = 0\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            \n",
    "            #*****************************************************************#\n",
    "            ##################    cross valid      ############################\n",
    "            #*****************************************************************#\n",
    "            val_start = 0\n",
    "            val_end = BATCH_SIZE\n",
    "            val_count =0\n",
    "            val_data_len = test_data.shape[0]\n",
    "            val_loss_tmp = 0\n",
    "            pre_right_num = 0\n",
    "            val_n_batch = val_data_len//BATCH_SIZE\n",
    "            if (count+1)%n_batch==0 and (count+1)//n_batch>n_StartCrossVal:\n",
    "                print(\"start test acc of val...\")\n",
    "                while val_count<val_n_batch:\n",
    "                    val_img =[]\n",
    "                    val_img_label = []\n",
    "                    for i in range(val_start,val_end):\n",
    "                        i = i%val_data_len\n",
    "                        img_file = os.path.join('data','train', test_data[i,1]) + '\\\\'+ test_data[i,2]\n",
    "                        img_tmp = read_img.get_img_sample(img_file,dataAug=dataAug)\n",
    "                        label_tmp = [int(test_data[i,1][1])]\n",
    "                        val_img.append(img_tmp)\n",
    "                        val_img_label.append(label_tmp)\n",
    "                    val_img = np.array(val_img).reshape(BATCH_SIZE,img_tmp.shape[1],img_tmp.shape[2],3)\n",
    "                    val_img_label = np.array(val_img_label).reshape(BATCH_SIZE)\n",
    "                    val_start = (val_start+BATCH_SIZE)%train_data_len\n",
    "                    val_end   = (val_end+BATCH_SIZE)%train_data_len\n",
    "                    if(val_end<val_start): val_end = val_start+BATCH_SIZE\n",
    "                    pre_cls,cls_loss_tmp= sess.run([cls_10_softmax,cls_loss],feed_dict={input_data:val_img,label:val_img_label,\n",
    "                                        keep_prob:1.0, learning_rate:0,L2_regu_rate:0.0})\n",
    "                    pre_label = np.argmax(pre_cls,axis=1)\n",
    "                    num_right = np.where(pre_label==val_img_label)[0].shape[0]\n",
    "                    pre_right_num += num_right\n",
    "                    val_loss_tmp += cls_loss_tmp\n",
    "                    val_count +=1\n",
    "                accury_tmp = float(pre_right_num)/float(BATCH_SIZE*val_n_batch)\n",
    "                val_loss_tmp = val_loss_tmp/val_n_batch\n",
    "                if val_loss<val_loss_tmp:\n",
    "                    num_acc_noupdate +=1\n",
    "                    print(\"loss don't down\",val_loss_tmp,\"pre accury is\",accury_tmp)\n",
    "                else:\n",
    "                    accury_max=accury_tmp\n",
    "                    val_loss = val_loss_tmp\n",
    "                    num_acc_noupdate=0\n",
    "                    saver.save(sess, save_checkpoint_dir + 'model.ckpt', global_step=count+1)\n",
    "                    print(\"save model\",count+1,\"loss update is\",val_loss_tmp,\"cross validation acc is\",accury_max)\n",
    "            count+=1\n",
    "    ##tensorboard --logdir  /path/to/Driver_Detection \n",
    "    return test_data,pre_cls,img_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc is  0.19691526263627354\n",
      "train acc is  0.7795961347869177\n",
      "train acc is  0.9193260654112984\n",
      "train acc is  0.9532581764122894\n",
      "train acc is  0.9686199207135778\n",
      "train acc is  0.9769697720515361\n",
      "start test acc of val...\n",
      "save model 24216 loss update is 0.0671651583611 cross validation acc is 0.9799107142857143\n",
      "train acc is  0.9816650148662042\n",
      "start test acc of val...\n",
      "save model 28252 loss update is 0.0510985596588 cross validation acc is 0.9860491071428571\n",
      "train acc is  0.9845515361744301\n",
      "start test acc of val...\n",
      "save model 32288 loss update is 0.0378190444945 cross validation acc is 0.9885044642857143\n"
     ]
    }
   ],
   "source": [
    "test_data,pre_cls,img_label = train(dataAug=True,checkpoint=False)\n",
    "img = test_data[:,2]\n",
    "subject = test_data[:,0]\n",
    "classname = test_data[:,1]\n",
    "dataframe = pd.DataFrame({'subject':subject,'classname':classname,'img':img})\n",
    "dataframe.to_csv('cross_validation11.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p042', 'p012', 'p072', 'p024', 'p021', 'p041', 'p016', 'p045', 'p061', 'p066', 'p002', 'p015', 'p052', 'p014', 'p026', 'p056', 'p049', 'p050', 'p035', 'p075', 'p047', 'p039']\n"
     ]
    }
   ],
   "source": [
    "train_c0 = ['p042','p012', 'p072', 'p024', 'p021', 'p041', 'p016', 'p045', 'p061', 'p066', 'p002', 'p015', 'p052', 'p014', 'p026', 'p056', 'p049', 'p050', 'p035', 'p075', 'p047', 'p039']\n",
    "test_c0 = ['p064','p051', 'p081', 'p022']\n",
    "\n",
    "train_c1 = [ 'p064','p051', 'p081', 'p022', 'p021', 'p041', 'p016', 'p045', 'p061', 'p066', 'p002', 'p015', 'p052', 'p014', 'p026', 'p056', 'p049', 'p050', 'p035', 'p075', 'p047', 'p039']\n",
    "test_c1 = ['p042','p012', 'p072', 'p024']\n",
    "\n",
    "train_c2 = [ 'p042','p012', 'p072', 'p024','p064','p051', 'p081', 'p022',  'p061', 'p066', 'p002', 'p015', 'p052', 'p014', 'p026', 'p056', 'p049', 'p050', 'p035', 'p075', 'p047', 'p039']\n",
    "test_c2 = ['p021', 'p041', 'p016', 'p045']\n",
    "\n",
    "train_c3 = ['p064','p051', 'p081', 'p022', 'p021', 'p041', 'p016', 'p045','p042','p012', 'p072', 'p024',   'p052', 'p014', 'p026', 'p056', 'p049', 'p050', 'p035', 'p075', 'p047', 'p039']\n",
    "test_c3 = ['p061', 'p066', 'p002', 'p015']\n",
    "\n",
    "train_c4 = [ 'p064','p051', 'p081', 'p022','p021', 'p041', 'p016', 'p045','p042','p012', 'p072', 'p024',  'p061', 'p066', 'p002', 'p015',  'p049', 'p050', 'p035', 'p075', 'p047', 'p039']\n",
    "test_c4 = ['p052', 'p014', 'p026', 'p056']\n",
    "\n",
    "train_c5 = ['p052', 'p014', 'p026', 'p056', 'p064','p051', 'p081', 'p022','p021', 'p041', 'p016', 'p045','p042','p012', 'p072', 'p024',  'p061', 'p066', 'p002', 'p015', 'p047', 'p039']\n",
    "test_c5 = ['p049', 'p050', 'p035', 'p075']\n",
    "print(train_c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver1 = pd.read_csv('submisson/1_VGG16_cro98.csv')\n",
    "driver2 = pd.read_csv('submisson/5_VGG16_loss0.07.csv')\n",
    "driver3 = pd.read_csv('submisson/3_VGG16_cro98_dataAug.csv')\n",
    "driver = (driver1.values[:,:10] + driver2.values[:,:10] + driver3.values[:,:10])/3\n",
    "name = driver1.values[:,10]\n",
    "c0 = driver[:,0]\n",
    "c1 = driver[:,1]\n",
    "c2 = driver[:,2]\n",
    "c3 = driver[:,3]\n",
    "c4 = driver[:,4]\n",
    "c5 = driver[:,5]\n",
    "c6 = driver[:,6]\n",
    "c7 = driver[:,7]\n",
    "c8 = driver[:,8]\n",
    "c9 = driver[:,9]\n",
    "dataframe = pd.DataFrame({'img':name,'c0':c0,'c1':c1,'c2':c2,'c3':c3,'c4':c4,'c5':c5,'c6':c6,'c7':c7,'c8':c8,'c9':c9})\n",
    "dataframe.to_csv('8_VGG_test.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
